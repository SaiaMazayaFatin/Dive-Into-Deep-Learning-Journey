## Vectorization for Speed
When training models, we need to process thousands of data points at once. To do this efficiently, we use **vectorization**.

Instead of using slow Python for-loops to process one number at a time, we use **linear algebra libraries** (like PyTorch or NumPy) to perform operations on entire arrays (vectors) at once. These libraries are optimized to run calculations in parallel, making them much faster.

### Code Example: For-Loop vs. Vectorization
Let's compare two ways of adding two 10,000-dimensional vectors.

**1. The Setup**
First, we create two vectors ($a$ and $b$) filled with 1s.

```python
import torch
import time

n = 10000
a = torch.ones(n)
b = torch.ones(n)
```

**2. The Slow Way (For-Loop)** This method adds the numbers one-by-one.

```python
c = torch.zeros(n)
t = time.time()
for i in range(n):
    c[i] = a[i] + b[i]
print(f'For-loop: {time.time() - t:.5f} sec')
```

**3. The Fast Way (Vectorized)** This method adds the entire vectors in a single operation.

```python
t = time.time()
d = a + b
print(f'Vectorized: {time.time() - t:.5f} sec')
```

**Why Vectorize?**
- **Speed**: Vectorized operations are often 10x to 100x faster because they use highly optimized C or CUDA code under the hood
- **Readability**: The code is shorter and looks more like the mathematical formulas.
- **Less Error**: You don't have to manage loop indices ($i$), which reduces the chance of bugs.