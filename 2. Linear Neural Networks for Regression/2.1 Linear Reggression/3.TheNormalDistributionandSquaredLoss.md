## The Connection: Normal Distribution & Squared Loss

Why do we use **Squared Error** as our loss function? It isn't just an arbitrary choice. There is a deep mathematical link between **Linear Regression** and the **Normal Distribution** (also called the Gaussian distribution).

**1. The Normal Distribution Formula**

The probability density of a normal distribution is defined by its **mean** ($\mu$) and **variance** ($\sigma^2$):

$$p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (x - \mu)^2\right)$$

**Math Breakdown:**
- $x$: The data point we are evaluating.
- $\mu$ (mu): The average (center of the bell curve).
- $\sigma^2$ (sigma squared): The variance (how spread out the curve is).
- $\frac{1}{\sqrt{2 \pi \sigma^2}}$: A constant that ensures the total probability equals 1.
- $\exp(\dots)$: The natural exponential function ($e$ raised to a power).
- $(x - \mu)^2$: The squared distance from the mean. This ensures the value is always positive and penalizes outliers.

**2. Visualizing with Code**

Changing the **mean** shifts the curve left or right. Changing the **standard deviation** ($\sigma$) makes the curve taller/skinny or shorter/wide.

```python
import numpy as np
import math

def normal(x, mu, sigma):
    # Calculate the constant part
    p = 1 / math.sqrt(2 * math.pi * sigma**2)
    # Calculate the exponential part
    return p * np.exp(-0.5 * (x - mu)**2 / sigma**2)

# Example usage for a range of values
x = np.arange(-7, 7, 0.01)
params = [(0, 1), (0, 2), (3, 1)] # (mean, std) pairs
```

![Normal Distribution Curves](img/3.png)

**3. Why Squared Loss? (The Logic)**

We assume that the actual labels ($y$) in our data are created by a linear model plus some random noise ($\epsilon$):

$$y = \mathbf{w}^T \mathbf{x} + \epsilon$$

If we assume the noise $\epsilon$ follows a normal distribution with a mean of $0$, then the probability of seeing a specific $y$ given our inputs $\mathbf{x}$ is:

$$P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right)$$

**Maximum Likelihood Estimation (MLE)**

To find the best weights ($\mathbf{w}$) and bias ($b$), we want to maximize the probability of the whole dataset. This is the **Maximum Likelihood Principle**:

$$P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)} \mid \mathbf{x}^{(i)})$$

To make the math easier, we take the Negative Log of this probability (Negative Log-Likelihood):

$$-\log P(\mathbf y \mid \mathbf X) = \sum_{i=1}^n \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b\right)^2$$

**The Result**:
If we assume $\sigma$ is constant, the only part of this formula that changes when we tweak $\mathbf{w}$ and $b$ is the last part:

$$\sum_{i=1}^n (y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b)^2$$

This is exactly the **Sum of Squared Errors!** This proves that minimizing squared error is the same as finding the most likely parameters for a model with normal noise.